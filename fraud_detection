#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().system('pip install scikeras tensorflow')


# In[6]:


get_ipython().system('pip install --upgrade scikit-learn imbalanced-learn')


# In[2]:


get_ipython().system('pip install lightgbm')


# In[10]:


get_ipython().system('pip install optuna')


# In[16]:


# ==============================
# IMPORT LIBRARIES
# ==============================

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve
)
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
from imblearn.over_sampling import SMOTE
import optuna

# ==============================
# LOAD DATASET
# ==============================

data_path = r"C:\Users\Jayadeep\OneDrive\Desktop\Unified_mentor\data"  # Update if needed

all_files = sorted(os.listdir(data_path))
df_list = [pd.read_pickle(os.path.join(data_path, file)) for file in all_files[:30]]
df = pd.concat(df_list, ignore_index=True)

# âœ… Convert timestamps
df['TX_TIME_SECONDS'] = pd.to_numeric(df['TX_TIME_SECONDS'])
df['TX_TIME_DAYS'] = pd.to_numeric(df['TX_TIME_DAYS'])
df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])

print("âœ… Dataset Loaded! Shape:", df.shape)

# ==============================
# FEATURE ENGINEERING
# ==============================

# âœ… Customer-based features
df['avg_tx_per_customer'] = df.groupby('CUSTOMER_ID')['TX_AMOUNT'].transform('mean')

# âœ… Transaction velocity (last 1h, 3h, 6h)
for hours in [1, 3, 6]:
    df[f'tx_count_last_{hours}h'] = df.groupby('CUSTOMER_ID')['TX_DATETIME'].transform(
        lambda x: x.diff().dt.total_seconds().lt(hours * 3600).cumsum()
    )

# âœ… Merchant-based pattern (Transactions at the same terminal)
df['tx_count_same_terminal'] = df.groupby(['CUSTOMER_ID', 'TERMINAL_ID'])['TX_AMOUNT'].transform('count')

# âœ… Relative spend pattern
df['relative_spend'] = df['TX_AMOUNT'] / df['avg_tx_per_customer']

# âœ… Selecting Features
features = ['TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS',
            'avg_tx_per_customer', 'tx_count_last_1h', 'tx_count_last_3h',
            'tx_count_last_6h', 'tx_count_same_terminal', 'relative_spend']

X = df[features].fillna(0)
y = df['TX_FRAUD']

print("âœ… Features Selected:", features)
print("âœ… X Shape:", X.shape, "| y Shape:", y.shape)

# ==============================
# FIX CLASS IMBALANCE WITH SMOTE
# ==============================

# âœ… Compute fraud class imbalance
fraud_cases = y.sum()
non_fraud_cases = len(y) - fraud_cases
scale_pos_weight = non_fraud_cases / fraud_cases  # Important for LightGBM

# âœ… Apply SMOTE to oversample fraud cases
smote = SMOTE(sampling_strategy=0.3, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("âœ… After SMOTE: Fraud Cases =", sum(y_resampled), "Non-Fraud Cases =", len(y_resampled) - sum(y_resampled))

# ==============================
# TRAIN-TEST SPLIT
# ==============================

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# ==============================
# HYPERPARAMETER TUNING WITH OPTUNA
# ==============================

def objective(trial):
    params = {
        'num_leaves': trial.suggest_int('num_leaves', 20, 60),
        'max_depth': trial.suggest_int('max_depth', 5, 15),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),
        'scale_pos_weight': scale_pos_weight,
        'n_estimators': 100
    }

    model = lgb.LGBMClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, scoring="roc_auc", cv=5)
    return np.mean(scores)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)

best_lgb_params = study.best_params
best_lgb_params['scale_pos_weight'] = scale_pos_weight

print("\nâœ… Best LightGBM Parameters:", best_lgb_params)

# ==============================
# TRAIN MODELS (LightGBM & Logistic Regression)
# ==============================

log_reg = LogisticRegression(max_iter=500)
log_reg.fit(X_train, y_train)

final_lgbm = lgb.LGBMClassifier(**best_lgb_params)
final_lgbm.fit(X_train, y_train)

# ==============================
# FIND BEST THRESHOLD FOR EACH MODEL
# ==============================

def find_best_threshold(model, X_test, y_test):
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)

    # Optimize for best F1-score
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)
    best_threshold = thresholds[np.argmax(f1_scores)]

    print(f"âœ… Best Threshold Found: {best_threshold:.3f}")
    return best_threshold

best_threshold_lgbm = find_best_threshold(final_lgbm, X_test, y_test)
best_threshold_logreg = find_best_threshold(log_reg, X_test, y_test)

# ==============================
# MODEL SELECTION BY USER
# ==============================

chosen_model = input("\nWhich model do you want to use? (LGBM/LogReg): ").strip().lower()
if chosen_model == "lgbm":
    model = final_lgbm
    best_threshold = best_threshold_lgbm
elif chosen_model == "logreg":
    model = log_reg
    best_threshold = best_threshold_logreg
else:
    print("Invalid choice! Defaulting to LightGBM.")
    model = final_lgbm
    best_threshold = best_threshold_lgbm

# ==============================
# MANUAL FRAUD DETECTION INPUT
# ==============================

print("\nðŸ”¹ Enter Transaction Details:")
tx_amount = float(input("Transaction Amount: "))
tx_time_seconds = int(input("Transaction Time in Seconds: "))
tx_time_days = int(input("Transaction Time in Days: "))
avg_tx_per_customer = float(input("Average Transaction per Customer: "))
tx_count_last_1h = int(input("Transaction Count Last 1 Hour: "))
tx_count_last_3h = int(input("Transaction Count Last 3 Hours: "))
tx_count_last_6h = int(input("Transaction Count Last 6 Hours: "))
tx_count_same_terminal = int(input("Transaction Count at Same Terminal: "))
relative_spend = tx_amount / avg_tx_per_customer

# Create input DataFrame
input_data = pd.DataFrame([[tx_amount, tx_time_seconds, tx_time_days, avg_tx_per_customer,
                            tx_count_last_1h, tx_count_last_3h, tx_count_last_6h, 
                            tx_count_same_terminal, relative_spend]], 
                          columns=features)

# Predict fraud with optimized threshold
fraud_prob = model.predict_proba(input_data)[:, 1][0]
print("\nðŸ”¹ Fraud Probability:", fraud_prob)

if fraud_prob > best_threshold:
    print("ðŸš¨ ALERT: This transaction is likely fraudulent!")
else:
    print("âœ… Transaction seems normal.")


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





